{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke1XjIInhRtj",
        "outputId": "8ee8cae9-9a42-4a38-909c-f1662de77f54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Q-table:\n",
            "Q(A, Move): 0.0\n",
            "Q(A, Stay): 0.0\n",
            "Q(B, Move): 0.0\n",
            "Q(B, Stay): 0.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define MDP parameters\n",
        "num_states = 2\n",
        "num_actions = 2\n",
        "gamma = 0.8\n",
        "alpha = 0.5\n",
        "num_steps = 200\n",
        "\n",
        "# Initialize Q-table with zeros\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Initial state\n",
        "current_state = 0  # State A\n",
        "\n",
        "# Run Q-learning for 200 steps\n",
        "for step in range(num_steps):\n",
        "    # Choose action based on greedy behavior policy\n",
        "    current_action = np.argmax(Q[current_state])\n",
        "\n",
        "    # If there is a tie, prefer move\n",
        "    if np.sum(Q[current_state] == Q[current_state, current_action]) > 1:\n",
        "        current_action = 0  # Move\n",
        "\n",
        "    # Perform the selected action and observe the next state and reward\n",
        "    if current_action == 0:  # Move\n",
        "        next_state = 1 - current_state  # Switch state\n",
        "        reward = 0\n",
        "    else:  # Stay\n",
        "        next_state = current_state\n",
        "        reward = 1\n",
        "\n",
        "    # Update Q-value\n",
        "    Q[current_state, current_action] = (1 - alpha) * Q[current_state, current_action] + alpha * (\n",
        "        reward + gamma * np.max(Q[next_state])\n",
        "    )\n",
        "\n",
        "    # Move to the next state\n",
        "    current_state = next_state\n",
        "\n",
        "# Display the final action-value table in the format: state, action\n",
        "print(\"Final Q-table:\")\n",
        "for state in range(num_states):\n",
        "    for action in range(num_actions):\n",
        "        print(f\"Q({chr(65 + state)}, {['Move', 'Stay'][action]}): {Q[state, action]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define MDP parameters\n",
        "num_states = 2\n",
        "num_actions = 2\n",
        "gamma = 0.8\n",
        "alpha = 0.5\n",
        "epsilon = 0.5  # Epsilon for ε-greedy policy\n",
        "num_steps = 200\n",
        "\n",
        "# Initialize Q-table with zeros\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Initial state\n",
        "current_state = 0  # State A\n",
        "\n",
        "# Run Q-learning for 200 steps with ε-greedy policy\n",
        "for step in range(num_steps):\n",
        "    # Choose action based on ε-greedy behavior policy\n",
        "    if np.random.rand() < epsilon:\n",
        "        current_action = np.random.choice(num_actions)  # Uniformly choose between move and stay\n",
        "    else:\n",
        "        current_action = np.argmax(Q[current_state])\n",
        "\n",
        "    # If there is a tie, break ties arbitrarily\n",
        "    if np.sum(Q[current_state] == Q[current_state, current_action]) > 1:\n",
        "        current_action = np.random.choice(num_actions)\n",
        "\n",
        "    # Perform the selected action and observe the next state and reward\n",
        "    if current_action == 0:  # Move\n",
        "        next_state = 1 - current_state  # Switch state\n",
        "        reward = 0\n",
        "    else:  # Stay\n",
        "        next_state = current_state\n",
        "        reward = 1\n",
        "\n",
        "    # Update Q-value\n",
        "    Q[current_state, current_action] = (1 - alpha) * Q[current_state, current_action] + alpha * (\n",
        "        reward + gamma * np.max(Q[next_state])\n",
        "    )\n",
        "\n",
        "    # Move to the next state\n",
        "    current_state = next_state\n",
        "\n",
        "# Display the final action-value table in the format: state, action\n",
        "print(\"Final Q-table:\")\n",
        "for state in range(num_states):\n",
        "    for action in range(num_actions):\n",
        "        print(f\"Q({chr(65 + state)}, {['Move', 'Stay'][action]}): {Q[state, action]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrug0ge5nOX1",
        "outputId": "a28db9c2-6fe3-4007-c59d-39d08ba51b54"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Q-table:\n",
            "Q(A, Move): 3.998529314668884\n",
            "Q(A, Stay): 4.999016864747624\n",
            "Q(B, Move): 3.997546494916402\n",
            "Q(B, Stay): 4.998786252774844\n"
          ]
        }
      ]
    }
  ]
}